---
name: test-spec-generator
description: Analyzes feature/task specification files and generates PBT (property-based testing) and unit test specifications for each, appending them to the bottom of the file. Use when asked to "generate tests for tasks", "add test specs to feature files", "what tests should I write for these features", or any request to derive test coverage from implementation specs.
---

# Test Spec Generator

Analyzes feature/task specification files and appends concrete test specifications (PBT + unit tests) to each file. Remember to add note to use vitest on frontend and node:test on backend.

## Workflow

1. **Receive input**: User provides a directory path or list of files containing task/feature specs
2. **Read each file**: Parse the spec to extract implementation requirements, data models, edge cases, validation rules, and any existing PBT specs
3. **Analyze testable surface**: For each file, identify what needs testing across these categories
4. **Generate test specs**: Write concrete, implementable test descriptions
5. **Append to file**: Add a `## Generated Test Specifications` section at the bottom of each file

## Analysis Categories

For each feature file, extract and generate tests across these dimensions:

### 1. Property-Based Tests (PBT)

Look for **invariants** — things that must ALWAYS or NEVER be true regardless of input:

- Data integrity invariants (e.g., "cleaned files are never re-cleaned")
- Boundary preservation (e.g., "retention period is always respected")
- Idempotency (e.g., "running cleanup twice produces same result")
- State machine properties (e.g., "status transitions are valid")
- Round-trip properties (e.g., "serialize then deserialize returns original")

### 2. Unit Tests — Happy Path

Derive from the **functional requirements** section:

- Each numbered requirement = at least one test
- Each processing step = at least one test
- Each data transformation = input/output test

### 3. Unit Tests — Validation & Guards

Derive from **validation rules**:

- Each validation rule = one positive + one negative test
- Required fields present/missing
- Type constraints respected/violated
- Business rule enforcement

### 4. Unit Tests — Edge Cases

Derive from **edge cases** section AND infer additional ones:

- Empty inputs / collections
- Concurrent operations
- Failure/retry scenarios
- Boundary values (0, 1, max, max+1)
- Null/undefined for optional fields

### 5. Integration Indicators

Flag tests that require external dependencies (DB, storage, queues) — mark as `[integration]` so the implementer knows to mock or use test containers.

## Output Format

Append the following structure to each file:

```markdown
---

## Generated Test Specifications

> Auto-generated by test-spec-generator. Review and adjust before implementing.

### Property-Based Tests

- **PBT-001: [property name]**
  - Property: [invariant statement in plain English]
  - Generator: [what random inputs to generate]
  - Assertion: [what must always hold]
  - Example: [one concrete example]

### Unit Tests

#### Happy Path

- **UT-001: [test name]**
  - Given: [setup]
  - When: [action]
  - Then: [expected outcome]

#### Validation & Guards

- **UT-V01: [test name]**
  - Given: [setup with invalid input]
  - When: [action]
  - Then: [expected error/rejection]

#### Edge Cases

- **UT-E01: [test name]** `[integration]`
  - Given: [edge case setup]
  - When: [action]
  - Then: [expected behavior]

### Coverage Notes

- [any gaps, assumptions, or recommendations for the implementer]
```

## Key Principles

1. **Be concrete, not generic**: Don't write "test that it works". Write "Given a file with `createdAt` 25 hours ago and `messageId: null`, when cleanup runs, then `isCleaned` should be `true`."
2. **Derive from the spec**: Every test should trace back to a specific requirement, validation rule, or edge case in the source file.
3. **Flag what you can't test from the spec alone**: If the spec is ambiguous or missing info, note it in Coverage Notes.
4. **Respect existing PBT specs**: If the file already has a `PBT Specs` section, expand on those — don't duplicate or contradict them.
5. **Keep test IDs scoped per file**: PBT-001, UT-001, etc. restart per file.
6. **Prefer PBT over unit tests when an invariant exists**: If you can express it as "for all valid X, property P holds", prefer PBT.

## Execution Steps

When invoked:

```
1. List/glob the target files
2. For each file:
   a. Read full contents
   b. Check if "## Generated Test Specifications" already exists
      - If yes: ask user whether to overwrite or skip
   c. Extract: requirements, data models, validation rules, edge cases, existing PBT specs
   d. Generate test specs following the analysis categories
   e. Append the Generated Test Specifications section
   f. Report: "[filename]: added N PBT + M unit tests"
3. Print summary: "Processed X files. Total: N PBT specs, M unit tests."
```
